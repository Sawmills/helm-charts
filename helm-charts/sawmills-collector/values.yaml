# Number of replicas for the Sawmills collector
# Note: This setting is ignored when mode is set to "daemonSet"
replicaCount: 3

# Deployment mode for the collector
# Available modes:
# - deployment: Standard Kubernetes Deployment (default)
# - daemonSet: Runs on every node in the cluster (or every node matching nodeSelector)
# When using daemonSet mode:
# - replicaCount and autoscaling settings are ignored
# - Rolling update strategy is handled differently by Kubernetes
# - Useful for collecting metrics/logs from every node in the cluster
mode: deployment

# The name of the collector
collectorName: sawmills-collector
collectorId: sawmills-collector-id

# Rollout configuration
rollout:
  # Global rollout settings
  terminationGracePeriodSeconds: 10
  minReadySeconds: 5
  
  # Deployment strategy configuration
  # For Deployment mode:
  # - RollingUpdate (default): Gradually replaces old pods with new ones
  # - Recreate: Deletes all existing pods before creating new ones
  # For DaemonSet mode:
  # - RollingUpdate (default): Gradually replaces old pods with new ones
  # - OnDelete: Only replaces pods when they are deleted
  #
  # BACKWARD COMPATIBLE: This chart supports both legacy and new formats
  # NEW FORMAT (recommended, aligns with upstream OpenTelemetry collector):
  # EXAMPLE:
  # For a conservative rolling update with minimal disruption:
  # rollout:
  #   strategy: RollingUpdate
  #   rollingUpdate:
  #     maxUnavailable: 1
  #     maxSurge: 1
  #   minReadySeconds: 30
  #
  # For a recreate strategy (all pods replaced at once):
  # rollout:
  #   strategy: Recreate
  #   terminationGracePeriodSeconds: 30
  strategy: RollingUpdate
  rollingUpdate:
    # Maximum number of pods that can be unavailable during the update
    # Can be specified as a number or percentage (e.g., "25%" or "1")
    maxUnavailable: 33%
    
    # Maximum number of pods that can be scheduled above the desired number of pods
    # Can be specified as a number or percentage (e.g., "25%" or "1")
    # Only applicable for Deployment mode, not for DaemonSet
    maxSurge: 50%
  
  # LEGACY FORMAT SUPPORT:
  # If you have existing configurations using this format, they will continue to work:
  # rollout:
  #   strategy:
  #     maxUnavailable: 33
  #     maxSurge: 50
  #
  # The chart will automatically detect the legacy format and apply the correct configuration.
  #
  # Migration guide:
  # 1. Change "strategy:" from a map to a string value (e.g., "RollingUpdate")
  # 2. Move maxUnavailable and maxSurge under "rollingUpdate:" section
  # 3. Add "%" suffix to percentage values if desired
  #

  # Main collector specific settings
  main:
    probes:
      liveness:
        initialDelaySeconds: 5
        periodSeconds: 5
        failureThreshold: 5
      readiness:
        initialDelaySeconds: 5
        periodSeconds: 5

  # Telemetry collector specific settings
  telemetry:
    probes:
      liveness:
        initialDelaySeconds: 5
        periodSeconds: 5
        failureThreshold: 5
      readiness:
        initialDelaySeconds: 5
        periodSeconds: 5

  # HAProxy specific settings
  haproxy:
    probes:
      liveness:
        initialDelaySeconds: 5
        periodSeconds: 5
        failureThreshold: 5
      readiness:
        initialDelaySeconds: 5
        periodSeconds: 5

# Configuration for the Sawmills collector
quotamgmtprocessor:
  s3_bucket: sawmills-plat-ue1-prod-quotas
  folder_name: SAWMILLS_ORG

# Configure the shared API secret used by the collector for authentication
# apiSecret:
#   name: sawmills-secret
#   key: api-key

prometheusremotewrite:
  endpoint: https://ingress.sawmills.ai
  

# extra environment variables for the collector
# for example:
# - name: DD_API_KEY
#   valueFrom:
#     secretKeyRef:
#       name: datadog
#       key: api-key
extraEnv: []
telemetryExtraEnv: []

# The image pull policy the Sawmills collector
image:
  pullPolicy: IfNotPresent
  tag: 1.558.0


# The node selector for the Sawmills collector
nodeSelector: {}

# Tolerations for the Sawmills collector pods
# Example:
# - key: "key1"
#   operator: "Equal"
#   value: "value1"
#   effect: "NoSchedule"
tolerations: []

# Pod annotations for the Sawmills collector
# These annotations will be added to the pod template metadata
# Example:
# podAnnotations:
#   foo: bar
#   prometheus.io/scrape: "true"
podAnnotations: {}

# Node affinity settings for the Sawmills collector pods
# Default configuration spreads pods across nodes to ensure high availability
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - sawmills-collector-chart
          topologyKey: kubernetes.io/hostname

# Topology spread constraints for the Sawmills collector pods
# Example:
# topologySpreadConstraints:
#   - kubernetes.io/hostname
#   - topology.kubernetes.io/zone
topologySpreadConstraints: []

# The resources for the Sawmills collector
resources:
  requests:
    memory: 512Mi
    cpu: 250m
  limits:
    memory: 2.5Gi
    cpu: 3000m

# Priority class name for the collector pods
# This determines the scheduling priority of the pods
# Useful for ensuring collector pods are scheduled before other pods
# Example: system-node-critical, system-cluster-critical
priorityClassName: ""

telemetry:
  resources:
    requests:
      memory: 256Mi
      cpu: 250m
    limits:
      memory: 2.5Gi
      cpu: 3000m
  # Prometheus port for the telemetry collector, can be used by customers to scrape metrics
  prometheus:
    port: 19465

# Standard Kubernetes HPA (autoscaling/v2)
autoscaling:
  enabled: false
  minReplicas: 3
  maxReplicas: 50
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80
  # Add behavior configuration for faster scaling
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 120
      policies:
        - type: Percent
          value: 50
          periodSeconds: 120
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 25
          periodSeconds: 120

# KEDA-based autoscaling (separate from standard HPA)
keda:
  enabled: false  
  minReplicas: 1
  maxReplicas: 100
  pollingInterval: 30
  cooldownPeriod: 300
  scaling:
    prometheus:
      enabled: false
      metricType: Value
      metadata:
        serverAddress: http://prometheus:9090
        query: sum(rate(http_requests_total{deployment="my-deployment"}[2m]))
        threshold: "100.50"
        activationThreshold: "5.5"
    external:
      enabled: false
      metricType: Value
      metadata:
        scalerAddress: "sawmills-collector-keda-otel-scaler.sawmills.svc.cluster.local:4418"
        query: "histogram_quantile(0.95, sum(rate(http_server_duration_bucket[2m])) by (le))"
        targetValue: "2000"
    cpu:
      enabled: true
      targetUtilization: 80
    memory:
      enabled: true
      targetUtilization: 80

# Ingress configuration
ingress:
  # Type of ingress to use (nginx or haproxy)
  type: nginx

  # Nginx ingress configuration
  nginx:
    enabled: false
    className: nginx
    port: 4318
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/ssl-redirect: "false"
    hosts:
      - host: sawmills-collector.local
        paths:
          - path: /
            pathType: Prefix
    tls: []
    # - secretName: collector-tls
    #   hosts:
    #     - sawmills-collector.local

  # HAProxy ingress configuration
  haproxy:
    enabled: false
    className: haproxy
    port: 4318
    annotations:
      kubernetes.io/ingress.class: haproxy
      haproxy.ingress.kubernetes.io/ssl-redirect: "false"
    hosts:
      - host: sawmills-collector.local
        paths:
          - path: /
            pathType: Prefix
    tls: []
    # - secretName: collector-tls
    #   hosts:
    #     - sawmills-collector.local

service:
  type: ClusterIP
  headless:
    enabled: true
  # internalTrafficPolicy controls how traffic from within the cluster is routed
  # Available values: "Cluster" (default) or "Local"
  # When set to "Local", traffic from within the cluster is routed to endpoints on the same node
  internalTrafficPolicy: ""
  # Service annotations
  # Example:
  # annotations:
  #   service.kubernetes.io/topology-mode: "Auto"  # Enables automatic topology-aware routing
  #   custom.annotation: "value"
  annotations: {}

# Additional containers to be added to the pod
# This allows you to add sidecar containers to the collector pod.
# Each container follows the Kubernetes container spec format.
# For detailed examples, see the examples directory.
#
# Example:
# additionalContainers:
#   sidecar:
#     image: busybox:1.35
#     command: ['sh', '-c', 'echo Hello && sleep 3600']
#     resources:
#       requests:
#         memory: "64Mi"
#         cpu: "100m"
#       limits:
#         memory: "128Mi"
#         cpu: "200m"
additionalContainers: {}

# Additional volumes to be mounted by additional containers
# This allows you to add volumes that can be mounted by sidecar containers.
# For detailed examples, see the examples directory.
#
# Example:
# additionalVolumes:
#   - name: config-volume
#     configMap:
#       name: sidecar-config
additionalVolumes: []

ports: {}
# Example of adding a port
# grpc:
#   protocol: TCP
#   port: 4317
# http:
#   protocol: TCP
#   port: 4318
# datadoglog:
#   protocol: TCP
#   port: 10518

# Configuration for the OpenTelemetry Collector
# You can either use S3-based configuration or direct configuration, but not both.
# S3-based configuration
otelConfig:
  # The S3 path where the collector configuration is stored (e.g., s3://my-bucket/collector-config.yaml)
  s3path: ""
  # The encryption key used to decrypt the configuration file
  encryptionKey: ""

# Direct configuration (used if otelConfig.s3path is not set)
otelCollectorConfig: {}
# Example configuration:
# receivers:
#   otlp:
#     protocols:
#       grpc:
#         endpoint: ${env:MY_POD_IP}:4317
# ... rest of the example configuration ...

# processors:
#   memory_limiter:
#     check_interval: 1s
#     limit_mib: 820
#     spike_limit_mib: 100

#   quotamgmtprocessor:
#     error_mode: ignore
#     s3_bucket: ${env:S3_BUCKET}
#     folder_name: ${env:FOLDER_NAME}
#     quota_refresh_duration: 60s
#     s3_region: us-east-1

#   throughputprocessor:
#     enabled: true
#     unique_label_value_count: 1000

#   batch:
#     send_batch_max_size: 1000
#     send_batch_size: 100
#     timeout: 10s

# exporters:
#   debug:
#     verbosity: basic

# extensions:
#   # The health_check extension is mandatory for this chart.
#   # Without the health_check extension the collector will fail the readiness and liveliness probes.
#   # The health_check extension can be modified, but should never be removed.
#   health_check:
#     endpoint: ${env:MY_POD_IP}:13133

#   cgroupruntime:
#     gomaxprocs:
#       enabled: true
#     gomemlimit:
#       enabled: true
#       ratio: 0.8

# service:
#   extensions: [health_check, cgroupruntime]
#   telemetry:
#     logs:
#       level: info
#       encoding: json
#     metrics:
#       readers:
#         - periodic:
#             interval: 50
#             timeout: 25
#             exporter:
#               otlp:
#                 protocol: grpc
#                 endpoint: 127.0.0.1:14317
#                 temporality_preference: delta
#                 compression: gzip

#   # Memlimit processor should be the first processor in the pipeline
#   # https://github.com/open-telemetry/opentelemetry-collector/tree/main/processor/memorylimiterprocessor#best-practices

#   pipelines:
#     logs:
#       receivers: [otlp, datadoglog]
#       processors: [memory_limiter, throughputprocessor, quotamgmtprocessor, batch]
#       exporters: [debug]
#     metrics:
#       receivers: [otlp]
#       processors: [memory_limiter, batch]
#       exporters: [debug]
#     traces:
#       receivers: [otlp]
#       processors: [memory_limiter, throughputprocessor, quotamgmtprocessor, batch]
#       exporters: [debug]

# HAProxy configuration
haproxy:
  enabled: false
  image: public.ecr.aws/docker/library/haproxy:3.1
  max_connections: 60000
  error_limit: 3

  resources:
    requests:
      memory: 128Mi
      cpu: 50m
    limits:
      memory: 256Mi
      cpu: 100m

  # Optional: Override global section
  global: {}
  # Example:
  #   log: "stdout format raw local0 info"
  #   maxconn: 4096
  #   nbthread: 4
  #   daemon: true

  # Optional: Override defaults section
  defaults: {}
  # Example:
  #   mode: "http"
  #   timeout:
  #     connect: "5s"
  #     client: "5s"
  #     server: "5s"
  #   retries: 3
  #   log: "global"
  #   options:
  #     - "redispatch"
  #     - "httplog"

  # Prometheus metrics configuration
  prometheus:
    enabled: true
    port: 8405 # Default prometheus port
    # Logging is always disabled for prometheus frontend

  # Stats page configuration
  stats:
    enabled: true
    port: 8406
    uri: "/"
    refresh: "10s"
    auth: "admin:admin"
    # Logging is always disabled for stats frontend

  # Healthcheck configuration
  healthcheck:
    enabled: true
    port: 13135

  # Port mapping configuration
  mapping:
    {}
    # Example:
    # http_4318:
    #   from: 4318
    #   to:
    #     port: 4318
    #     fallback_endpoint: datadoghq.com
    #   mode: "http"
    #   fallback_enabled: true
    #   logging: false        # Whether to enable logging for this frontend (default: true)
    #   options:
    #     - "httplog"
    #     - "dontlognull"
    #   backend_options:
    #     - "httpchk"
    #     - "allbackups"

haproxyConfig:
  receivers:
    prometheus:
      config:
        scrape_configs:
          - job_name: haproxy
            scrape_interval: 15s
            static_configs:
              - targets: ["${env:MY_POD_IP}:8405"]
  service:
    pipelines:
      metrics/prometheus:
        receivers: [prometheus]
        exporters: [prometheusremotewrite]

telemetryConfig:
  receivers:
    otlp/customer_collector:
      protocols:
        grpc:
          tls:
            cert_file: /etc/otel/certs/server-cert.pem
            key_file: /etc/otel/certs/server-key.pem
            ca_file: /etc/otel/certs/ca-cert.pem
          endpoint: 127.0.0.1:14317
          max_recv_msg_size_mib: 10
    otlp/telemetry_collector:
      protocols:
        grpc:
          tls:
            cert_file: /etc/otel/certs/server-cert.pem
            key_file: /etc/otel/certs/server-key.pem
            ca_file: /etc/otel/certs/ca-cert.pem
          endpoint: 127.0.0.1:14318
          max_recv_msg_size_mib: 10

  exporters:
    debug:
      # verbosity: detailed
      verbosity: basic
    prometheus:
      endpoint: ${env:MY_POD_IP}:${env:PROMETHEUS_PORT}
      metric_expiration: 1m

    prometheusremotewrite:
      endpoint: ${env:PROMETHEUS_REMOTE_WRITE_ENDPOINT}
      headers:
        X-API-KEY: ${env:PROMETHEUS_REMOTE_WRITE_API_KEY}
      external_labels:
        pod_name: ${env:MY_POD_NAME}
        collector_name: ${env:COLLECTOR_NAME}
        collector_id: ${env:COLLECTOR_ID}
  processors:
    memory_limiter:
      check_interval: 1s
      limit_mib: 0
      limit_percentage: 95
      spike_limit_mib: 0
      spike_limit_percentage: 10

    batch:
      metadata_cardinality_limit: 0
      metadata_keys: []
      send_batch_max_size: 2048
      send_batch_size: 1024
      timeout: 1s

    transform/remove_unit:
      error_mode: ignore
      metric_statements:
        - context: metric
          statements:
            - set(metric.unit, "")
            - set(resource.attributes["service.name"], "otel-collector")

    transform/mark_as_telemetry:
      error_mode: ignore
      metric_statements:
        - context: metric
          statements:
            - set(resource.attributes["service.name"], "otel-collector-telemetry")
    deltatocumulative:
      max_stale: 1m

  extensions:
    cgroupruntime:
      gomaxprocs:
        enabled: true
      gomemlimit:
        enabled: true
        ratio: 0.95
    health_check:
      # we're using port 13134 for healthcheck of the telemetry collector (vs 13133 for the main collector)
      endpoint: ${env:MY_POD_IP}:13134

  connectors:
    routing:
      error_mode: ignore
      table:
        # pass all metrics that are not related to thp or qmp to system pipeline
        - context: metric
          statement: route() where not IsMatch(name, "otelcol_processor_central_thp|otelcol_processor_thp|otelcol_processor_qmp")
          pipelines: [metrics/system]
    forward:

  service:
    extensions: [health_check, cgroupruntime]
    pipelines:
      metrics/customer_collector:
        receivers: [otlp/customer_collector]
        processors:
          - memory_limiter
          - transform/remove_unit # TODO: This should be moved to the `metrics/system` pipeline once we start sending metrics with the new format
          - deltatocumulative # TODO: This will be moved to `metrics/system` once we start sending metrics with the new format
          - batch
        exporters: [routing, forward]

      metrics/telemetry_collector:
        receivers: [otlp/telemetry_collector]
        processors:
          - memory_limiter
          - transform/remove_unit
          - transform/mark_as_telemetry
          - deltatocumulative
          - batch
        exporters: [routing, forward]

      metrics/system:
        receivers: [routing]
        exporters: [prometheus]

      metrics/external:
        receivers: [forward]
        exporters: [prometheusremotewrite]

    telemetry:
      resource:
        pod: ${env:MY_POD_NAME}
      logs:
        encoding: json
        error_output_paths:
          - stdout
        level: info
        output_paths:
          - stdout
      metrics:
        readers:
          - periodic:
              interval: 15000 # 15 seconds
              timeout: 30000 # 30 seconds
              exporter:
                otlp:
                  protocol: grpc
                  endpoint: 127.0.0.1:14318
                  certificate: /etc/otel/certs/ca-cert.pem
                  temporality_preference: delta
                  compression: gzip


# deprecated, use serviceAccount.name instead
serviceAccountName: ""
serviceAccount:
  # -- Whether to create a service account for the Sawmills Collector deployment.
  create: false
  # -- Additional labels to add to the created service account.
  additionalLabels: {}
  # -- Annotations to add to the created service account.
  annotations: {}
  # -- The name of the existing service account to use when
  # serviceAccount.create is false.
  name: null

# ServiceMonitor configuration for Prometheus Operator
serviceMonitor:
  # Enable or disable ServiceMonitor resource creation
  enabled: false
  # Additional labels for the ServiceMonitor
  labels:
    release: prometheus
  # Metrics endpoints configuration
  metricsEndpoints:
    - port: prometheus
      interval: 15s

kedaScaler:
  enabled: false
  imagePullPolicy: Always
  podSecurityContext: {}
  probes:
    liveness:
      initialDelaySeconds: 5
      periodSeconds: 5
      failureThreshold: 5
    readiness:
      initialDelaySeconds: 5
      periodSeconds: 5
  service:
    type: ClusterIP
    otlpReceiverPort: 4518
    kedaExternalScalerPort: 4418
    healthcheckPort: 13134
    monitoringPort: 19465
    otelPrometheusPort: 19876
  resources:
    limits:
      cpu: 500m
      memory: 256Mi
    requests:
      cpu: 500m
      memory: 128Mi
  telemetryConfig:
    receivers:
      prometheus/keda_scaler:
        config:
          scrape_configs:
            - job_name: keda_scaler
              scrape_interval: 15s
              static_configs:
                - targets: ["${env:MY_POD_IP}:${env:KEDA_SCALER_OTEL_PROMETHEUS_PORT}"]
    processors:
      filter/keda:
        error_mode: ignore
        metrics:
          metric:
            - name != "http.server.duration"
    exporters:
      otlp/keda:
        endpoint: sawmills-collector-keda-otel-scaler.sawmills.svc.cluster.local:${env:KEDA_SCALER_OTLP_RECEIVER_PORT}
        compression: "none"
        tls:
          insecure: true
    service:
      pipelines:
        metrics/telemetry_keda_scaler:
          receivers: [prometheus/keda_scaler]
          processors:
            - memory_limiter
            - batch
          exporters: [routing, forward]
        metrics/keda:
          exporters:
            - otlp/keda
          processors:
            - filter/keda
          receivers:
            - forward
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:${env:OTLP_RECEIVER_PORT}
    exporters:
      kedascaler:
        engine_settings:
          timeout: 30s
          max_samples: 500000
          lookback_delta: 5m
          retention_duration: 10m
        monitoring_http:
          endpoint: ${env:MY_POD_IP}:${env:MONITORING_PORT}
        scaler_grpc:
          endpoint: ${env:MY_POD_IP}:${env:KEDA_EXTERNAL_SCALER_PORT}
          transport: tcp
    processors:
      memory_limiter:
        check_interval: 1s
        limit_mib: 0
        limit_percentage: 95
        spike_limit_mib: 0
        spike_limit_percentage: 10
      batch:
        metadata_cardinality_limit: 0
        metadata_keys: []
        send_batch_max_size: 2048
        send_batch_size: 1024
        timeout: 1s
    extensions:
      cgroupruntime:
        gomaxprocs:
          enabled: true
        gomemlimit:
          enabled: true
          ratio: 0.95
      health_check:
        endpoint: ${env:MY_POD_IP}:${env:HEALTHCHECK_PORT}
    service:
      extensions:
        - health_check
        - cgroupruntime
      pipelines:
        metrics:
          receivers:
            - otlp
          processors:
            - memory_limiter
            - batch
          exporters:
            - kedascaler
      telemetry:
        resource:
          pod: ${env:MY_POD_NAME}
        logs:
          encoding: json
          error_output_paths:
            - stdout
          level: info
          output_paths:
            - stdout
        metrics:
          address: ${env:MY_POD_IP}:${env:OTEL_PROMETHEUS_PORT}
